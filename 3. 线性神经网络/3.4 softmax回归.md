**1.1** 计算 $softmax$交叉熵损失 $l(\bf{y,\hat{y}})$ 的二阶导数

![image](https://github.com/Mushero/d2l_HomeWork/blob/main/3.%20%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img/1.png)

一阶导数为： $l^{\prime}(\bf{y,\hat{y}}) = softmax(\bf{o_j})-y_j$ 

二阶导数为： $l^{\prime\prime}(\bf{y,\hat{y}}) = [softmax(\bf{o_j})-y_j]^{\prime}$
<br/>可得 $ = {\frac{exp(o_k)}{\sum_{k=1}^{q}exp(o_k)}}^{\prime}$ 可得 $ = \frac{exp(o_k){\sum_{k=1}^{q}exp(o_k)}-exp(o_k)exp(o_k)}{[{\sum_{k=1}^{q}exp(o_k)}]^2} = softmax(o_j)(1-softmax(o_j))$
![image](https://github.com/Mushero/d2l_HomeWork/blob/main/3.%20%E7%BA%BF%E6%80%A7%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/img/2.png)

**3.1** 证明：

$\because e^a > 0,\quad e^b > 0$	

$\therefore RealSoftMax(a,b)=\log(e^a+e^b) > \log(e^{max(a,b)}) = max(a,b)$

即 $RealSoftMax(a,b) > max(a,b)$

**3.2** 证明：

$\because e^{\lambda a} > 0, \quad e^{\lambda b} > 0, \quad \lambda > 0$

$\therefore \lambda^{-1} RealSoftMax(\lambda a,\lambda b) = \lambda^{-1}\log(e^{\lambda a} + e^{\lambda b})>\lambda^{-1}\log(e^{max(\lambda a,\lambda b)}) = \log(e^{\lambda max(a,b)})^{\lambda^{-1}} = max(a,b)$

即 $\lambda^{-1} RealSoftMax(\lambda a,\lambda b) > max(a,b),\quad (\lambda>0)$

**3.3** 证明：

当 $\lambda \to \infty$时， $e^{\lambda a} + e^{\lambda b} \to e^{max(\lambda a,\lambda b)}$

故 $\lambda \to \infty$时， $\lambda^{-1} RealSoftMax(\lambda a,\lambda b) \to \lambda^{-1}\log(e^{max(\lambda a,\lambda b)}) = max(a,b)$
 
即 $\lambda^{-1} RealSoftMax(\lambda a,\lambda b) \to max(a,b),\quad (\lambda \to \infty)$

**3.4** $softmin(x) = softmax(-x)$

**3.5** $RealSoftMax(n_1,n_2,n_3,\cdots n_n) = \log(e^{n_1}+e^{n_2}+e^{n_3}+\cdots+e^{n_n})$
